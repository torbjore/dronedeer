---
title: "Results"
date: now
author: Torbjørn Ergon
number-sections: true
toc: true
toc-depth: 4
format:
  html:
    embed-resources: true
---

```{r}
#| echo: false
#| output: false
Sys.setlocale(locale='no_NB.utf8') # Include for using Norwegian letters in plots
library(knitr)
library(coda)
source("utilities.r")
```

DETTE ER ET INTERNT ARBEIDSDOKUMENT - IKKE MENT FOR PUBLISERING (men skriver på engelsk av vane og for eventuelt gjenbruk)

SISTE OPPDATERING:

* Bruker nå bare det jeg før kalte "super wide priors" for 'mu'.
* Har retta opp feil i data

OPPSUMMERING *FØR* SISTE ZOOM-MØTE (kopiert fra e-post):

* Gamma-modellen (den beste WAIC-modellen) hadde vi formulert på en litt innfløkt måte og vi hadde også gjort det litt feil. Jeg forsøkte å gjøre det på en mye enklere måte, men da fikk jeg problemer med særdeles dårlig mixing i mcmc-kjedene fra NIMBLE som jeg ikke fant ut av (jeg viste det til Daniel Turek som har laget NIMBLE, og som jeg kjenner godt – han syntes det var interessant, men fant heller ikke noen rask løsning). Long-story-short, jeg fant til slutt en slags reparameterisering av modellen (med en annen variance-mean relationship som lignet på den måten vi først gjorde det (bare riktig denne gangen)) som fungerte. All good.

* Jeg gjorde ‘posterior predictive checks’ på modellene. Disse viste at det er betydelig mer variasjon i y-matrisa (antallet sett av obs1, ob2 og begge for hvert site) enn det man skal forvente utfra modellen. For å hanskes med dette innførte jeg en random effekt på deteksjonssansynlighetene. Dette løste problemet, men ga naturligvis enda litt bredere konfidensintervaller (eller «credible intervals» for å være mer korrekt) – men ikke så mye som man kunne frykte. All good.

* Jeg har gjort noen sensitivitetsanalyser – bl.a. sett på betydningen av ulike priors og overlapp mellom priors og posteriors. Dette viste at det var urovekkende høy overlapp mellom prior og posterior for noen av parameterene. For noen av parameterene var dette forventet for vi har bruk priors som er noe informative – spesielt for deteksjonssannsynlighetene der vi har brukt informasjon fra flyvningene over inngjerdingene. For mu0 (median tetthet ved gjennomsnittlig avstand fra innmark) brukte vi også litt informative priors basert på enkle estimater av deteksjonssannsynlighet og antall hjort sett. Vi gjorde dette helt fra starten av for å beregne noen rimelige startverdier og for å få en raskere konvergens i mcmc-kjøringene. Jeg regna likevel med at vi satt priors såpass vide at de hadde liten innvirkning på resultatet – men det er dessverre ikke tilfelle; for m0 er overlap mellom prior og posterior mellom 60% (for områder med mest data) til 98% (!) for områder det det ikke er sett hjort i det hele tatt. Så for områdene der det ikke er sett hjort får vi altså posterior-fordelinger som er så å si helt lik priors, og for de andre områdene er overlappen også urovekkende stor (vi burde selvsagt ha sjekka dette tidligere). Jeg har sjekka med simulering at vi får smalere konfidensintervaller som er mye smalere med mye mer data, så jeg tror ikke det er noe iboende i modellen som gjør noen parametere uidentifiserbare


REFERAT FRA SISTE ZOOM-MØTE (kopiert fra e-post):

Her er noen oppsummeringspunkter – og noen tanker videre som vi kan huske på i skrivingen (kommenter gjerne):

* Vi ble enige om å presentere resultater fra tilpassingen med essensielt ikke-informative priors for hjortetettheter (til sist i den siste versjonen av dokumentet dere fikk). Men i den biologiske tolkningen av resultatet kan vi legge mest vekt på nedre «konfidens-grense» (2.5% kvantilen av posteriori-fordelingen) – som et slags minimumsestimat. For de 4 områdene er disse 20, 0, 16, og 0.8 deer/km2 i mars og 28, 0, 1.5 og 0 deer/km2 i april. Jeg kan også beregne 1% og 0.1% kvantiler. Disse viser jo at tetthetene i alle fall lokalt er vesentlig høyere enn «management target» på én hjort per km2. Siden vi bare har 4 områder og ikke har flydd mer enn max 500 m fra innmark, kan vi ikke si noe om gjennomsnittlig tetthet over noe større områder.

* Som «proof of concept» kan vi presentere resultat fra simuleringer med 500 sites per område. Selv om konfidensintervallene her blir mye smalere får vi fortsatt ganske usikre estimater. Vi har høy deteksjonssannsynlighet, med gode estimater, så de nokså usikre tetthetsestimatene skyldes nok at hjorten er nokså klumpvis fordelt (det er stor variasjon i tetthet mellom ‘sites’ (fokusbilde-«foot prints») innen områder, i tillegg til stor variasjon mellom områder).  Man kan potensielt fly enda høyere, slik at hvert bilde dekker et større område og man får både høyere antall hjort per site og lavere variasjon mellom sites – men jeg er da redd for at man da vil få vesentlig lavere oppdagbarhet og ikke minst lavere «availability» siden man ikke får bilde av bakken fra ulike vinkler på samme måten når man flyr veldig høyt. Det er en stor fordel å kunne regne med at availability er tilnærmet 1 slik vi har indikasjoner på at den er (gitt at vi betrakter flere bilder av samme sites fra ulike vinkler). Jeg tror derfor Julie har flydd på en ganske optimal høyde.


## Overview of raw data

```{r}
#| echo: false
# Loading data
load(file = "../data/LDDdata.rda")
```

Number of sites (focal images) per 'sample-area-and-month' ('sam'):

```{r}
#| echo: false
N_sites <- apply(LDDdata$data$Y, 1, function(i) sum(!is.na(i))) |> tapply(INDEX = LDDdata$const$sam, sum)
sam_names <- c("Haugen (April)", "Haugen (March)", "Raa (April)", "Raa (March)", "Søre Bjørkum (April)", "Søre Bjørkum (March)", "Sprakehaug (April)", "Sprakehaug (March)")
names(N_sites) <- sam_names
N_sites
```

Total area per sample-area-and-month (unit: hectare = % of 1 km$^2$)

```{r}
#| echo: false
area_per_sam <- apply(LDDdata$const$area, 1, sum, na.rm=TRUE) |> tapply(INDEX = LDDdata$const$sam, sum)
names(area_per_sam) <- sam_names
area_per_sam
```

Numbers of deer seen per 'sample-area-and-month' ('sam'):

```{r}
#| echo: false
Y_per_sam <- apply(LDDdata$data$Y, 1, sum, na.rm=TRUE) |> tapply(INDEX = LDDdata$const$sam, sum)
names(Y_per_sam) <- sam_names
Y_per_sam
```

Frequency distribution of number of deer seen per site (focal image):

```{r}
table(LDDdata$data$Y)
```
...275 sites with zero deer seen, 9 sites with 1 deer seen, etc.

Variance divided by the mean is

```{r}
#| echo: false
var(as.vector(LDDdata$data$Y), na.rm = TRUE)/mean(as.vector(LDDdata$data$Y), na.rm = TRUE)
```

This is quite much higher than expected from a Poisson distribution (where var/mean = 1). Some of this will be explained by the fixed effects in the model (area-and-month and distance from the field), and the rest will be accounted for by the random effects. Seeing the high number of zeros, one could be tempted to try a model with zero-inflation, but I would find this model to be a a bit strange (would there be a lot of sites that never would have any deer?? I don't think so...) - so I haven't tried this. 

Simple point estimate of detectability based on how many of those seen by obs1 is seen by obs2 and vice versa:

```{r}
colsumy <- apply(LDDdata$data$y, 3, sum, na.rm=TRUE)
p1hat <- colsumy[3]/(colsumy[2]+colsumy[3]) # = 0.70
p2hat <- colsumy[3]/(colsumy[1]+colsumy[3]) # = 0.90
(phat_simple <- 1 - (1-p1hat)*(1-p2hat))
```
This is the probability that at least one of the two observers registered a given deer in the picture (obs1 has probability 0.7 and obs2 has probability 0.9).

Simple density estimates per 'sam' (unit = number of deer per hectare)

```{r}
(lambdahat <- (Y_per_sam/phat_simple)/area_per_sam)
```

The estimates per km$^2$ are 100 times this.

We can use the logarithms of these values to construct priors for 'mu0' (the log of median densities at mean distance from the field). The sam's where there are zero observations pose a problem. We cannot say much in terms of a point estimate or a lower confidence/credible interval in these cases, but there is some information about the upper confidence/credible interval (i.e, we can get some idea about how high the densities can potentially be for these areas with zero counts). To use the same prior-structure as for the other sams I computed prior mean densities from mcmc-sampling with a simple Poisson-model as specified in the script 'prior_zero_sites.r'

<!-- Initial models fitted to the data suggest that 'sigma' in the lognormal and gamma models is about 1.47, which means that the median is quite much lower than the mean (only about 1/3). -->


<!-- ```{r} -->
<!-- log(lambdahat/exp(0.5*1.47^2)) -->
<!-- ``` -->

To construct priors for 'mu0', I have used a normal distribution with the standard deviation (sigma) selected such that the 97.5% quantile of the prior distribution for median density among sites, exp(mu0), is 100 times higher than the 2.5% quantile of the prior distribution. However, as is shown below, with this prior we get very substantial overlap between prior and posterior distributions. Hence, I also tried setting sigma such that the 2.5 percentile for the prior distribution for density is 1/100'th of the median and the 97.5 percentile is 100 times the median (i.e., the relative difference between these two percentiles is 10 000). This leads to lower overlap, but the credible intervals are also naturally wider. I'm a bit unsure how we should present this and would like to discuss.  

## Checking convergence and WAIC models

```{r}
#| echo: false

# WAIC can be used even when we have different observation models: https://discourse.mc-stan.org/t/can-waic-looic-be-used-to-compare-models-with-different-likelihoods/7380

Path <- "../data/posterior_samples"
files_all <- dir(path = Path)
files <- files_all[grep(pattern = "pos|LONG", files_all, invert=TRUE)]
models <- c(
  "exponential_0",
  "lognormal_0",
  "gamma_0",
  "exponential_1",
  "lognormal_1",
  "gamma_1",
  "exponential_2",
  "lognormal_2",
  "gamma_2"
  )

df <- data.frame(model = models, WAIC = rep(NA, length(models)))
for(mod in models){
  cat("\n **", mod, "**\n")
  fls <- files[grep(mod, files)]
  postsamp <- WAIC <- NULL
  for(fl in fls){
    load(paste0("../data/posterior_samples/", fl))
    postsamp <- c(postsamp, out$samples)
    WAIC <- c(WAIC, out$WAIC$WAIC)
  }
  postsamp <- as.mcmc.list(postsamp)
  cat("Number of runs:", length(fls), "\n")
  cat("Settings for each run:\n")
  print(unlist(settings))
  cat("\nConvergence diagnostics for all chains combined:\n")
  print(gelman.diag(postsamp))
  cat("\nWIAC in each run:", WAIC)
  cat("\nMean WAIC:", mean(WAIC), "\n")
  df$WAIC[df$model == mod] <- mean(WAIC)
}

kable(df)
```

There are some parameters that have problems with convergence in the exponential models now that I didn't have when I used more restrictive parameters, I will have to look closer at that to see if it can be improved. However, since these models were far from having the lowest WAIC also when convergence was good (more restricted priors), I'm sure these model will not get the lowest WAIC even with good convergence - this can also be assessed from the differences in the WAIC values between the runs (each run has 3 chains).

## Convergence diagnistics from the long run of the gamma_2 model

```{r}
#| echo: false
# Loading LONG RUN of the best model
best <- "gamma_2"
fls <- files_all[grep("gamma_2_LONG", files_all)]
best_postsamp <- NULL
for(fl in fls){
  load(paste0("../data/posterior_samples/", fl))
  best_postsamp <- c(best_postsamp, out$samples)
}
best_postsamp <- as.mcmc.list(best_postsamp)

gelman.diag(best_postsamp)

```

## Posterior predictive checks of best model

Posterior predictive checks on initial models showed that we need random effects on detection probabilities to get acceptable fit (see summary in beginning of the document). I here only show the Bayesian p-value plots for the final best model.

### Bayesian p-value based on Y

```{r}
#| echo: false
samp <- as.matrix(best_postsamp)
plot(samp[,"Disc_New_Y"] ~ samp[,"Disc_Y"])
abline(0, 1, col="red")
mean(samp[, "Disc_New_Y"] > samp[, "Disc_Y"])
```

### Bayesian p-value based on y

```{r}
#| echo: false

plot(samp[,"Disc_New_y"] ~ samp[,"Disc_y"])
abline(0, 1, col="red")
mean(samp[, "Disc_New_y"] > samp[, "Disc_y"])
```

### Conclusion

No indication of serious lack of fit.

## Summary of best model

```{r}
#| echo: false
best_postsamp <- best_postsamp[,-(1:4)]
summary(best_postsamp)
```


## Predictions from the best model

### Plot

```{r}
#| echo: false
#| warning: FALSE

library(ggplot2)
#library(hrbrthemes) # TE> Tar bort dette for jeg har ikke alle fontene som kreves installert på min maskin ser det ut til
library(cowplot)

#############
# Preparing #
#############

load(file = "../data/UseData.rda")
load(file = "../data/Counts.rda")

sam_names <- c("Haugen (April)", "Haugen (March)", "Raa (April)", "Raa (March)", "Søre Bjørkum (April)", "Søre Bjørkum (March)", "Sprakehaug (April)", "Sprakehaug (March)")

predictor_variable <- UseData$mean_field_dist
Xlab <- "Distance from field (m)"
Ylab <- expression(Density~of~deer~(km^-2))

X <- seq(0, max(predictor_variable, na.rm = TRUE), length.out=50)
X_mean <-  mean(predictor_variable, na.rm=TRUE)
X_sd <- sd(predictor_variable, na.rm=TRUE)
X_st <- (X - X_mean)/X_sd

mpv <- mean(predictor_variable, na.rm = TRUE) # mean predictor variable
pap <- median(-samp[,"beta[1]"]/(2*samp[,"beta[2]"]))*X_sd + X_mean # predictor variable at peak

############
# Plotting #
############

plots <- pred_plots(PS = samp)

plot_grid(plots[[2]],
          plots[[1]],
          plots[[4]],
          plots[[3]])
plot_grid(plots[[6]],
          plots[[5]],
          plots[[8]],
          plots[[7]])
```

* Mean distance from the field (stippled line): `r round(mpv, 0)` meters.
* Posterior median of distance at peak deer density (dotted line): `r round(pap, 0)` meters.

Posterior summary for distance at peak deer density:

```{r}
#| echo: FALSE

quantile(-samp[,"beta[1]"]/(2*samp[,"beta[2]"]), probs = c(0.025, 0.5, 0.975))*X_sd + X_mean

```

### Table - Predictions at peak deer density (for table A3):

* Kommentar: Jeg har her brukt samme rekkefølge som i figuren (den rekkefølgen du har på plottene i manus), men table A3 har en annen rekkefølge. Jeg synes du skal bruke samme rekkefølge i figur og tabell. Kanskje du også skal forenkle å bare ta med posterior median, 2.5 percentil og 97.5 percentil - det er det jeg har gjort i plottet nå. (Denne tabellen (eller den under) bør kanskje heller være i hovedteksen og ikke i Appendix?)

Density units: number of deer per km$^2$.

x = distance from field

```{r}
#| echo: FALSE

df <- data.frame(SAM = sam_names, x = pap)
pap_st <- (pap - X_mean)/X_sd
for(SAM in 1:length(sam_names)){
  y <- pred(pap_st, PS = samp, sam = SAM)
  Q <- quantile(y, probs = c(0.025, 0.5, 0.975))
  df$mean[SAM] <- mean(y)
  df$median[SAM] <- Q[2]
  df$lwr[SAM] <- Q[1]
  df$upr[SAM] <- Q[3]
}

kable(df[c(2,1,4,3,6,5,8,7), ], digits = 2)

```

### Table - Predictions at mean distance from the field:

```{r}
#| echo: FALSE

df <- data.frame(SAM = sam_names, x = mpv)
mpv_st <- (mpv - X_mean)/X_sd
for(SAM in 1:length(sam_names)){
  y <- pred(mpv_st, PS = samp, sam = SAM)
  Q <- quantile(y, probs = c(0.025, 0.5, 0.975))
  df$mean[SAM] <- mean(y)
  df$median[SAM] <- Q[2]
  df$lwr[SAM] <- Q[1]
  df$upr[SAM] <- Q[3]
}

kable(df[c(2,1,4,3,6,5,8,7), ], digits = 2)
```

Konfidensintervallene blir jo her smallere. Jeg forelsår vi presenterer bare denne istedenfor.

## Overlap with prior

Overlap between prior distribution and posterior distribution (multiply by 100 to get % overlap):

```{r}
#| echo: FALSE
samp <- samp[,-(1:4)]
df = data.frame(par = dimnames(samp)[[2]],
                sam = c(NA, NA, sam_names, rep(NA, 4)),
                overlap = 0)

#1-2 (beta):
lambdahat <- LDDdata$const$lambdahat
for(i in 1:2){
  df$overlap[i] <- post_prior_overlap(samp[,i], dnorm, mean = 0, sd = 2)
}

#3-10 (mu0):
lambdahat <- LDDdata$const$lambdahat
for(i in 3:10){
  df$overlap[i] <- post_prior_overlap(samp[,i], dnorm, mean = 0, sd = 10) #log(lambdahat[i-2]/exp(0.5*1.47^2)), sd = 1.175)
}

#11-12 (mu_p1 and mu_p2)
load(file = "../data/prior_parameters_for_p.rda")
for(i in 11:12){
  df$overlap[i] <- post_prior_overlap(samp[,i], dnorm, mean = prior_parameters_for_p$mu_logit_p, sd = prior_parameters_for_p$sigma_logit_p)
}

#13 (sigma)
df$overlap[13] <- post_prior_overlap(samp[,13], dunif, min = 0.5, max = 3)

#14 (sigma_p)
df$overlap[14] <- post_prior_overlap(samp[,14], dunif, min = 0.1, max = 0.59)

df
```

Overlaps for 'mu_p1' and 'mu_p2' are high because we used informative priors based on the enclosure surveys. The high overlap for 'sigma_p' is also expected (I've explained this in the methods-appendix).


Plot:

* black = posterior
* red = prior

```{r}
#| echo: FALSE
#| layout-ncol: 2
#| layout-nrow: 4

for(i in 3:10){
  nouse <- post_prior_overlap(samp[,i], dnorm, mean = 0, sd = 10, plt = TRUE,
                                      plt.main = paste0(df$par[i], " (overlap: ", round(df$overlap[i], 3), ")")
                                      )
}
```

We can also compute the overlap for predicted median density at mean distance from field (i.e., for exp(mu)). These are a bit different than for mu, depending on whether the overlap is largest to the left or the right of the distribution:

```{r}
#| echo: FALSE

# Overlap for predicted density at mean distance from field (i.e., for exp(mu))
df = data.frame(par = paste0('exp(', dimnames(samp)[[2]][3:10], ')'),
                sam = sam_names,
                overlap = 0)
for(i in 1:8){
  df$overlap[i] <- post_prior_overlap(exp(samp[,i+2]), dlnorm, meanlog = 0, sdlog = 10)
}

df
```

## Predictions from the constant gamma model

The constant gamma model has only slightly higher WAIC: Computing the predictions from this model:

```{r}
#| echo: FALSE

# Loading
const_fls <- files[grep("gamma_0", files)]
const__postsamp <- NULL
for(fl in const_fls){
  load(paste0("../data/posterior_samples/", fl))
  const_postsamp <- c(const__postsamp, out$samples)
}
const_postsamp <- as.mcmc.list(const_postsamp)
const_samp <- as.matrix(const_postsamp)

df <- data.frame(SAM = sam_names)
#mpv_st <- (mpv - X_mean)/X_sd
for(SAM in 1:length(sam_names)){
  y <- pred(x=0, PS = const_samp, sam = SAM)
  Q <- quantile(y, probs = c(0.025, 0.5, 0.975))
  df$mean[SAM] <- mean(y)
  df$median[SAM] <- Q[2]
  df$lwr[SAM] <- Q[1]
  df$upr[SAM] <- Q[3]
}

kable(df[c(2,1,4,3,6,5,8,7), ], digits = 2)
```

## Some sensitivity assessment

### Choice of radom distribution for $\lambda$

Predictions from each of the models at mean distance from field at Haugen in March:

```{r}
#| echo: false

i = 1
df <- data.frame(Model = models)
for(mod in models){
  fls <- files[grep(mod, files)]
  postsamp <- NULL
  for(fl in fls){
    load(paste0("../data/posterior_samples/", fl))
    postsamp <- c(postsamp, out$samples)
  }
  postsamp <- as.mcmc.list(postsamp) |> as.matrix()
  y <- pred(x=0, PS = postsamp, sam = 2)
  Q <- quantile(y, probs = c(0.025, 0.5, 0.975))
  df$mean[i] <- mean(y)
  df$median[i] <- Q[2]
  df$lwr[i] <- Q[1]
  df$upr[i] <- Q[3]
  i <- i+1
}
print(df)
```

### Influence of restrictive prior for $\sigma_p$

NB! This has not (yet) been updated with "superwide" priors and corrected data.

Summaries from a model where I increased the upper limit of the the random effects SD for detection log-odds from 0.59 to 5.

```{r}
#| echo: FALSE

load("../data/posterior_samples/OLD/gam_2_test.RData")
gelman.diag(out$samples)
#plot(out$samples)
summary(out$samples)

samp <- as.matrix(out$samples)
plots <- pred_plots(PS = samp)

plot_grid(plots[[2]],
          plots[[1]],
          plots[[4]],
          plots[[3]])
plot_grid(plots[[6]],
          plots[[5]],
          plots[[8]],
          plots[[7]])

df <- data.frame(SAM = sam_names, x = pap)
#pap_st <- (pap - X_mean)/X_sd
for(SAM in 1:length(sam_names)){
  y <- pred(x = 0, PS = samp, sam = SAM)
  Q <- quantile(y, probs = c(0.025, 0.5, 0.975))
  df$mean[SAM] <- mean(y)
  df$median[SAM] <- Q[2]
  df$lwr[SAM] <- Q[1]
  df$upr[SAM] <- Q[3]
}

print(df[c(2,1,4,3,6,5,8,7), ])

```

Predictions above are at the mean distance from the field.

The results are, perhaps surprisingly, similar although the 'sigma_p' parameter is estimated very high. I think it is strange that 'sigma_p' is estimated so high, but good to see that it makes such a small difference of the results.

## Proof of concept - Indentifiablity

NB! This has not (yet) been updated with "superwide" priors and corrected data.

To check that there are no inherent identifiability problems with the model, I fitted the model to simulated data; 8 areas with 500 sites. Parameters set to posterior mean of original fit. Same priors as original (IKKE "superwide" - bør oppdatere).

```{r}
#| echo: FALSE

load("../data/posterior_samples/sim/sim_gamma_2_500_sites.RData")
#gelman.diag(out)
#plot(out$samples)
summary(out)

samp <- as.matrix(out)
plots <- pred_plots(PS = samp)

plot_grid(plots[[2]],
          plots[[1]],
          plots[[4]],
          plots[[3]])
plot_grid(plots[[6]],
          plots[[5]],
          plots[[8]],
          plots[[7]])
```

Predictions at peak density:

```{r}
#| echo: FALSE
df <- data.frame(SAM = sam_names, x = pap)
pap_st <- (pap - X_mean)/X_sd
for(SAM in 1:length(sam_names)){
  y <- pred(pap_st, PS = samp, sam = SAM)
  Q <- quantile(y, probs = c(0.025, 0.5, 0.975))
  df$mean[SAM] <- mean(y)
  df$median[SAM] <- Q[2]
  df$lwr[SAM] <- Q[1]
  df$upr[SAM] <- Q[3]
}

kable(df[c(2,1,4,3,6,5,8,7), ], digits = 2)
```

Predictions at mean distance from field:

```{r}
#| echo: FALSE

df <- data.frame(SAM = sam_names, x = mpv)
mpv_st <- (mpv - X_mean)/X_sd
for(SAM in 1:length(sam_names)){
  y <- pred(mpv_st, PS = samp, sam = SAM)
  Q <- quantile(y, probs = c(0.025, 0.5, 0.975))
  df$mean[SAM] <- mean(y)
  df$median[SAM] <- Q[2]
  df$lwr[SAM] <- Q[1]
  df$upr[SAM] <- Q[3]
}

kable(df[c(2,1,4,3,6,5,8,7), ], digits = 2)
```

Overlap with prior:

```{r}
#| echo: FALSE
df = data.frame(par = dimnames(samp)[[2]],
                sam = c(NA, NA, sam_names, rep(NA, 4)),
                overlap = 0)

#1-2 (beta):
lambdahat <- LDDdata$const$lambdahat
for(i in 1:2){
  df$overlap[i] <- post_prior_overlap(samp[,i], dnorm, mean = 0, sd = 2)
}

#3-10 (mu0):
lambdahat <- LDDdata$const$lambdahat
for(i in 3:10){
  df$overlap[i] <- post_prior_overlap(samp[,i], dnorm, mean = log(lambdahat[i-2]/exp(0.5*1.47^2)), sd = 1.175)
}

#11-12 (mu_p1 and mu_p2)
load(file = "../data/prior_parameters_for_p.rda")
for(i in 11:12){
  df$overlap[i] <- post_prior_overlap(samp[,i], dnorm, mean = prior_parameters_for_p$mu_logit_p, sd = prior_parameters_for_p$sigma_logit_p)
}

#13 (sigma)
df$overlap[13] <- post_prior_overlap(samp[,13], dunif, min = 0.5, max = 3)

#14 (sigma_p)
df$overlap[14] <- post_prior_overlap(samp[,14], dunif, min = 0.1, max = 0.59)

print(df)
```

CONCLUSION: With this much data (8 areas with 500 sites each), the credible intervals do get a lot narrower, so there doesn't seem to be any inherent no non-identifiable issues. Overlaps with prior distributions are still high - there would naturally be lower overlap if I used wider priors, but credible intervals will then also become somewhat wider (especially for the areas with low densities where the overlap is the highest). Below I have fitted the model to the observed data with much wider priors.

One would perhaps expect higher precision (narrower credible intervals) when having 8 $\times$ 500 sites but the high variation in number of deer per sites contributes to much uncertainty, and since the average area of sites is just under 0.6 ha (0.6% of a km$^2$), the expected counts per km$^2$ is quite small.